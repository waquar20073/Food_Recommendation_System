{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based Collaborative Filtering Algorithm Based on Group Weighted Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used these 4 libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math, joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uar(userId, train_pt_df):\n",
    "    l = [x for x in list(train_pt_df.loc[userId]) if x != 0]\n",
    "    if len(l)==0:\n",
    "        return 2.5\n",
    "    else:\n",
    "        return sum(l)/len(l)\n",
    "\n",
    "\n",
    "def iar(itemId, train_pt_df):\n",
    "    l = [x for x in list(train_pt_df[itemId]) if x != 0]\n",
    "    if len(l)==0:\n",
    "        return 2.5\n",
    "    else:\n",
    "        return sum(l)/len(l)\n",
    "\n",
    "\n",
    "def func(train_pt_df):\n",
    "    train_pt_df = train_pt_df.T\n",
    "    \n",
    "    user_average_rating = {userId:uar(userId, train_pt_df) for userId in train_pt_df.index}\n",
    "    item_average_rating = {itemId:iar(itemId, train_pt_df) for itemId in train_pt_df.columns}\n",
    "    \n",
    "    git = {}\n",
    "    for item in (train_pt_df.columns):\n",
    "        l = train_pt_df[item].tolist()\n",
    "        M = 3\n",
    "        rp = [i for i in l if i >= M and i != 0]\n",
    "        rq = [i for i in l if i < M and i != 0]\n",
    "        left = 0\n",
    "        right = 0\n",
    "        for i in rp:\n",
    "            left += i-M\n",
    "        for i in rq:\n",
    "            right += i-M\n",
    "        git[item] = left - right\n",
    "    \n",
    "    gitd = {}\n",
    "    for item in (train_pt_df.columns):\n",
    "        if git[item] > 0:\n",
    "            l = train_pt_df[item].tolist()\n",
    "            M = 3\n",
    "            rp = [i for i in l if i >= M and i != 0]\n",
    "            if len(rp) == 0:\n",
    "                gitd[item] = 0\n",
    "            else:\n",
    "                gitd[item] = sum(rp)/len(rp)\n",
    "        elif git[item] == 0:\n",
    "            gitd[item] = 3\n",
    "        else:\n",
    "            l = train_pt_df[item].tolist()\n",
    "            M = 3\n",
    "            rq = [i for i in l if i < M and i != 0]\n",
    "            if len(rq) == 0:\n",
    "                gitd[item] = 0\n",
    "            else:\n",
    "                gitd[item] = sum(rq)/len(rq)\n",
    "        \n",
    "    new_train_pt_df = train_pt_df\n",
    "\n",
    "    for item in tqdm(train_pt_df.columns):\n",
    "        for user in train_pt_df.index:\n",
    "            if train_pt_df[item][user] == 0:\n",
    "    #             r = [x for x in list(train_pt_df[item]) if x != 0]\n",
    "    #             ri_bar = item_average_rating[item]\n",
    "    #             r = [i-ri_bar for i in r]\n",
    "    #             ru = user_average_rating[user] + (sum(r)/len(r))\n",
    "\n",
    "    #             u = [x for x in list(train_pt_df.iloc[user-1]) if x != 0]\n",
    "    #             ui_bar = user_average_rating[user]\n",
    "    #             u = [i-ui_bar for i in u]\n",
    "    #             ri = item_average_rating[item] + (sum(u)/len(u))\n",
    "                one = user_average_rating[user]\n",
    "                two = item_average_rating[item]\n",
    "                three = gitd[item]\n",
    "                new_train_pt_df[item][user] = three #math.sqrt(three)\n",
    "    return new_train_pt_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_IB(userId, itemId, train_pt_df, sm_df, K):\n",
    "    try:\n",
    "        users_ratings = train_pt_df[userId]  # ratings of all items for user 'userId'\n",
    "        items_similarities = sm_df[itemId]  # cosine similarities of item 'itemId' with all other items.\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    # Consider only highest K item similarities. (Here, similarity of the same item will also not considered).\n",
    "    d = dict(items_similarities) # {itemId : item_similarity}\n",
    "    d_sorted = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    drop_rest_users = list(d_sorted.keys())\n",
    "    drop_rest_users = drop_rest_users[K:]\n",
    "    users_ratings = users_ratings.drop(drop_rest_users)\n",
    "    items_similarities = items_similarities.drop(drop_rest_users)\n",
    "    \n",
    "    # Not consider items whose ratings are NA.\n",
    "    drop_empty_ratings = users_ratings[users_ratings == 0].index\n",
    "    users_ratings = users_ratings.drop(drop_empty_ratings)\n",
    "    items_similarities = items_similarities.drop(drop_empty_ratings)\n",
    "    \n",
    "    global coverage\n",
    "    # # If I encountered with coverage problem i.e, no similar items exist after threshold. Then take aveage rating of the movie.\n",
    "    if len(users_ratings) == 0 or len(items_similarities) == 0:\n",
    "        coverage = coverage + 1\n",
    "        return -1\n",
    "    else:\n",
    "        # Normalize all the similarities of all the items.\n",
    "        items_similarities_sum = sum(items_similarities)\n",
    "        items_similarities = [i/items_similarities_sum for i in items_similarities]\n",
    "\n",
    "        # Linearly interpolate the Active User 'userId' rated items by corresponding normalized similarities.\n",
    "        linear_interpolation = np.dot(users_ratings, items_similarities)\n",
    "\n",
    "        # return this predicted rating.\n",
    "        if math.isnan(linear_interpolation) ==  False:\n",
    "            return linear_interpolation\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the training & testing dataframes and return the MAE.\n",
    "def IB_MAE(train_df, test_df, K):\n",
    "    # pivot the training dataframe and Transpose it.\n",
    "    train_pt_df = pd.pivot_table(train_df, values='Rating', index='userId', columns='itemId').T\n",
    "    # Replace the NA values with 0   (Note: I observed -> No user in the whole dataset have rated 0 to any movie)\n",
    "    train_pt_df = train_pt_df.fillna(0)\n",
    "    \n",
    "    train_pt_df = func(train_pt_df)\n",
    "    \n",
    "    # Calculate the cosine similarities of item-item.\n",
    "    sm_df = pd.DataFrame(cosine_similarity(train_pt_df), index=train_pt_df.index, columns=train_pt_df.index)\n",
    "    # Not consider the similarity of same item while predicting the rating. eg: similarity of item 1 with item 1.\n",
    "    np.fill_diagonal(sm_df.values, 0)\n",
    "    \n",
    "    # Actual Ratings of testing dataframe.\n",
    "    actual_ratings  = list(test_df['Rating'])\n",
    "    # Start predicting the ratings of testing dataframe.\n",
    "    predicted_ratings = []\n",
    "    for userId, itemId in tqdm(zip(test_df['userId'], test_df['itemId'])):\n",
    "        predicted_ratings.append(predict_rating_IB(userId, itemId, train_pt_df, sm_df, K))\n",
    "        \n",
    "    # Ignore the case when predicted rating is -1. (Because no such items are available to predict the rating. #Coverage_Problem)\n",
    "    new_actual_ratings = []\n",
    "    new_predicted_ratings = []\n",
    "    for i in range(0, len(predicted_ratings)):\n",
    "        if predicted_ratings[i] <= 0:\n",
    "            continue\n",
    "        else:\n",
    "            new_actual_ratings.append(actual_ratings[i])\n",
    "            new_predicted_ratings.append(predicted_ratings[i])\n",
    "    \n",
    "    # return the MAE between Actual Ratings & Predicted Ratings.\n",
    "    return mean_absolute_error(new_actual_ratings, new_predicted_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2817/2817 [16:46<00:00,  2.80it/s]\n",
      "3021it [00:27, 109.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.8073645224326266 , Coverage = 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Dataset/reviews.csv')\n",
    "df = df.rename(columns={'RecipeId': 'itemId', 'AuthorId': 'userId'})\n",
    "\n",
    "d = (dict(enumerate(df['userId'].unique())))\n",
    "new_d = {}\n",
    "new_d = {value:key for key, value in d.items()}\n",
    "df['userId'] = df['userId'].map(new_d)\n",
    "\n",
    "d = (dict(enumerate(df['itemId'].unique())))\n",
    "new_d = {}\n",
    "new_d = {value:key for key, value in d.items()}\n",
    "df['itemId'] = df['itemId'].map(new_d)\n",
    "\n",
    "train, test = train_test_split(df[(df['userId']<10000) & (df['itemId']<3000)], test_size=0.2)\n",
    "\n",
    "coverage = 0\n",
    "mae_value = IB_MAE(train, test, 800)   # Returning MAE for each fold.\n",
    "print(\"MAE =\", mae_value, \", Coverage = \" + str(100-((coverage*100)/test.shape[0])) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
